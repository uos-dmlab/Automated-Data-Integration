{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f39d8e",
   "metadata": {},
   "source": [
    "# CNE-join : 조인 가능한 테이블을 찾아주는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ede691",
   "metadata": {},
   "outputs": [],
   "source": [
    "##개체명 인식기는 https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a 를 참고하여 만듦\n",
    "import pandas as pd\n",
    "df = pd.read_csv('ner.csv')\n",
    "# Split labels based on whitespace and turn them into a list\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "\n",
    "#필요한 모듈 임포트\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75d6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext 모델 임포트\n",
    "from gensim.models import fasttext\n",
    "\n",
    "ft_model = fasttext.load_facebook_model(\"cc.en.300.bin\")\n",
    "\n",
    "# 개체명인식 모델 임포트\n",
    "model = torch.load(\"model_alldata.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac66993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_list(model, arr):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    #geo for geographical entity / org for organization entity / per for person entity / gpe for geopolitical entity / tim for time indicator entity / art for artifact entity / eve for event entity / nat for natural phenomenon entity / O is assigned if a word doesn’t belong to any entity.\n",
    "    ner_dict = {'B-geo':0, 'B-gpe':0, 'B-per':0, 'B-tim':0, 'B-eve':0, 'B-art':0, 'B-nat':0, 'B-org':0}\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for sentence in arr:\n",
    "        text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        mask = text['attention_mask'].to(device)\n",
    "        input_id = text['input_ids'].to(device)\n",
    "        label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "        logits = model(input_id, mask, None)\n",
    "        logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1).tolist()\n",
    "        prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "        for label in prediction_label:\n",
    "            if label in ner_dict :\n",
    "                ner_dict[label] +=1 \n",
    "    return [k for k,v in ner_dict.items() if max(ner_dict.values()) == v]\n",
    "    \n",
    "def containment(arr1,arr2):\n",
    "    #모든 값 소문자로 변경\n",
    "    larr1 = set([str(i).lower() for i in arr1])\n",
    "    larr2 = set([str(i).lower() for i in arr2])\n",
    "    \n",
    "    threshold = 0.6\n",
    "    denominator = min(len(larr1), len(larr2))\n",
    "    numerator = len(larr1.intersection(larr2))\n",
    "    \n",
    "    if (numerator / denominator) > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#한 테이블의 모든 컬럼에 대해 각 컬럼의 unique한 값으로 column nmae + is + column value의 문장을 만들어 그 결과를 리스트에 담아 return 하는 함수\n",
    "#이를 활용해 각 컬럼의 NE를 결정한다\n",
    "def make_sent(tab1, col1):\n",
    "    sent1 = []\n",
    "    for word in tab1[col1].unique():\n",
    "        sent1.append(col1 + \" is \" + str(word))\n",
    "\n",
    "    return sent1\n",
    "\n",
    "# 두 컬럼에 대해 column_name embedding 값 반환\n",
    "def column_name_emb(col1, col2):\n",
    "    return ft_model.wv.similarity(col1,col2)\n",
    "\n",
    "def joinable_table(tab1, tab2):\n",
    "    #threshold 값 정의\n",
    "    col_th = 0.3\n",
    "    \n",
    "    #각 테이블에서 수치형 컬럼 전부 드랍\n",
    "    drop_col1 = []\n",
    "    drop_col2 = []\n",
    "    for i in range(len(tab1.columns)):\n",
    "        col_name1 = tab1.columns[i]\n",
    "        if is_numeric_dtype(tab1[col_name1]):\n",
    "            drop_col1.append(col_name1)\n",
    "    for drop in drop_col1:        \n",
    "        tab1.drop(columns=[drop],inplace=True)\n",
    "        \n",
    "    for i in range(len(tab2.columns)):\n",
    "        col_name2 = tab2.columns[i]\n",
    "        if is_numeric_dtype(tab2[col_name2]):\n",
    "            drop_col2.append(col_name2)\n",
    "    for drop in drop_col2:        \n",
    "        tab2.drop(columns=[drop],inplace=True)\n",
    "    \n",
    "    #각 테이블의 모든 것에 컬럼들에 대해 컬럼쌍 만들기\n",
    "    col_comb = []\n",
    "    for n1 in tab1.columns:\n",
    "        for n2 in tab2.columns:\n",
    "            col_comb.append([n1,n2])\n",
    "    #print(col_comb)\n",
    "            \n",
    "        \n",
    "    #각 컬럼쌍들에 대해 column_name_embedding해서 1차 거르기 True / False\n",
    "    drop_CN = []\n",
    "    for comb in col_comb:\n",
    "        #print(comb)\n",
    "        if column_name_emb(comb[0],comb[1]) < col_th:\n",
    "            #print(comb)\n",
    "            drop_CN.append(comb)\n",
    "    #print(\"CN : \",col_comb)\n",
    "    #print(drop_CN)\n",
    "    for drop in drop_CN:\n",
    "        col_comb.remove(drop)\n",
    "            \n",
    "    #NER해서 2차 거르기\n",
    "    drop_NER = []\n",
    "    for comb in col_comb:\n",
    "        sent1 = make_sent(tab1,comb[0])\n",
    "        sent2 = make_sent(tab2,comb[1])\n",
    "        if evaluate_list(model, sent1) != evaluate_list(model, sent2): \n",
    "            drop_NER.append(comb)\n",
    "    for drop in drop_NER:\n",
    "        col_comb.remove(drop)\n",
    "    #print(\"NER : \",col_comb)\n",
    "    \n",
    "    #containment해서 최종 거르기\n",
    "    drop_cont = []\n",
    "    for comb in col_comb:\n",
    "        #print(comb)\n",
    "        if not (containment(tab1[comb[0]].unique(),tab2[comb[1]].unique()) or containment(tab2[comb[1]].unique(),tab1[comb[0]].unique())):\n",
    "            drop_cont.append(comb)\n",
    "    for drop in drop_cont:\n",
    "        col_comb.remove(drop)\n",
    "    \n",
    "    if len(col_comb) > 0 :\n",
    "        return col_comb\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e5f07a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\관계형테이블임베딩\\\\데이터셋\\\\2019_world_happiness.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Airports.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\android-games.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\android_games_224games.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\AQI and Lat Long of Countries.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Automobile.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Best Songs on Spotify from 2000-2023.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\bournemouth_venues.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\CARS_1.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Car_Models.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\chip_dataset.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\CO2_emission.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\coffee-listings-from-all-walmart-stores.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\commodity_prices.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\countries-table.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Covid Live.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Crop_recommendation.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Customertravel.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\df_arabica_clean.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\ds_salaries.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\europe.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Food_Preference.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Food_Production.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Fruit Prices 2020.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\genre_region_totals.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\global_transport.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Holiday_Package.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\housePrice.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\imdb_christmas_movies_2017-22.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\indian_food.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\kbopitchingdata.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\List of best-selling PlayStation 4 video games.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Netflix Revenue.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\nfl_stadiums.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\nfl_teams.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\nobel_latest.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\oscar.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\ramen-ratings.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\resultbook2.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\Salary_Data.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\ski_hotels.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\superbowl.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\TikTok_songs_2021.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\top50.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\top_defense_manufacturers.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\TravelInsurancePrediction.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\us_births_2016_2021.csv',\n",
       " 'C:\\\\관계형테이블임베딩\\\\데이터셋\\\\winequality-red.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "table_list = []\n",
    "path = 'C:\\관계형테이블임베딩\\데이터셋\\*.*'\n",
    "for file in glob.glob(path, recursive=True):\n",
    "    table_list.append(file)\n",
    "from itertools import combinations\n",
    "tab_comb = []\n",
    "for i in combinations(table_list,2):\n",
    "    tab_comb.append(i)\n",
    "tab_comb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0dd830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_world_happiness.csv CO2_emission.csv [['Country or region', 'Country Name']]\n",
      "2019_world_happiness.csv Covid Live.csv [['Country or region', 'Country,\\nOther']]\n",
      "2019_world_happiness.csv df_arabica_clean.csv [['Country or region', 'Country of Origin']]\n",
      "2019_world_happiness.csv global_transport.csv [['Country or region', 'Country Name']]\n",
      "2019_world_happiness.csv nobel_latest.csv [['Country or region', 'Organization_Country']]\n",
      "2019_world_happiness.csv top_defense_manufacturers.csv [['Country or region', 'country ']]\n",
      "Airports.csv AQI and Lat Long of Countries.csv [['Country', 'Country']]\n",
      "Airports.csv countries-table.csv [['Country', 'country']]\n",
      "Airports.csv ramen-ratings.csv [['Country', 'Country']]\n",
      "AQI and Lat Long of Countries.csv countries-table.csv [['Country', 'country']]\n",
      "AQI and Lat Long of Countries.csv ramen-ratings.csv [['Country', 'Country']]\n",
      "AQI and Lat Long of Countries.csv superbowl.csv [['City', 'City']]\n",
      "CARS_1.csv Car_Models.csv [['body_type', 'Body Type']]\n",
      "CO2_emission.csv Covid Live.csv [['Country Name', 'Country,\\nOther']]\n",
      "CO2_emission.csv df_arabica_clean.csv [['Country Name', 'Country of Origin']]\n",
      "CO2_emission.csv global_transport.csv [['Country Name', 'Country Name'], ['country_code', 'Country Code']]\n",
      "CO2_emission.csv nobel_latest.csv [['Country Name', 'Death_Country'], ['Country Name', 'Organization_Country']]\n",
      "CO2_emission.csv top_defense_manufacturers.csv [['Country Name', 'country ']]\n",
      "countries-table.csv ramen-ratings.csv [['country', 'Country']]\n",
      "Covid Live.csv df_arabica_clean.csv [['Country,\\nOther', 'Country of Origin']]\n",
      "Covid Live.csv global_transport.csv [['Country,\\nOther', 'Country Name']]\n",
      "Covid Live.csv nobel_latest.csv [['Country,\\nOther', 'Organization_Country']]\n",
      "Customertravel.csv global_transport.csv [['AnnualIncomeClass', 'Country Name']]\n",
      "Customertravel.csv TravelInsurancePrediction.csv [['FrequentFlyer', 'FrequentFlyer'], ['AccountSyncedToSocialMedia', 'EverTravelledAbroad']]\n",
      "df_arabica_clean.csv global_transport.csv [['Country of Origin', 'Country Name']]\n",
      "ds_salaries.csv nobel_latest.csv [['company_location', 'Birth_Country_Code']]\n",
      "Food_Preference.csv Holiday_Package.csv [['Gender', 'Gender']]\n",
      "Food_Preference.csv nobel_latest.csv [['Gender', 'Gender']]\n",
      "Food_Preference.csv Salary_Data.csv [['Gender', 'Gender']]\n",
      "global_transport.csv nobel_latest.csv [['Country Name', 'Death_Country'], ['Country Name', 'Organization_Country']]\n",
      "global_transport.csv top_defense_manufacturers.csv [['Country Name', 'country ']]\n",
      "Holiday_Package.csv nobel_latest.csv [['Gender', 'Gender']]\n",
      "Holiday_Package.csv Salary_Data.csv [['Gender', 'Gender']]\n",
      "nobel_latest.csv Salary_Data.csv [['Gender', 'Gender']]\n",
      "superbowl.csv us_births_2016_2021.csv [['State', 'State']]\n"
     ]
    }
   ],
   "source": [
    "for tables in tab_comb:\n",
    "    try:\n",
    "        tab1 = pd.read_csv(tables[0],encoding='ISO-8859-1')\n",
    "    except pd.errors.ParserError:\n",
    "        tab1 = pd.read_csv(tables[0],encoding='ISO-8859-1',sep=';')    \n",
    "    try:\n",
    "        tab2 = pd.read_csv(tables[1],encoding='ISO-8859-1')\n",
    "    except pd.errors.ParserError:\n",
    "        tab2 = pd.read_csv(tables[1],encoding='ISO-8859-1',sep=';')\n",
    "     \n",
    "    \n",
    "    tab1.dropna(axis=1,how='all',inplace=True)\n",
    "    tab2.dropna(axis=1,how='all',inplace=True)\n",
    "    if joinable_table(tab1, tab2) != False:\n",
    "        print(tables[0].split(\"\\\\\")[-1],tables[1].split(\"\\\\\")[-1] ,joinable_table(tab1, tab2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed081318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
